import re
import json
import traceback
import textwrap

def generate_etl_code(source_metadata, target_metadata, target_platform, llm, use_agentic=False, mapping_metadata=None, dq_metadata=None):
    """
    Generates complete PySpark code for ETL based on metadata using LLM.
    Includes transformation rules, data quality filters, and error segregation.
    """
    print("Generating ETL code...")
    if source_metadata.empty or target_metadata.empty:
        return "Error: Missing source or target metadata."

    transformation_logic = ""
    if mapping_metadata is not None:
        try:
            mapping_dict = mapping_metadata[[
                "Business Rule / Expression",
                "Join Conditions",
                "Filter Conditions",
                "Lookup Table Used",
                "Aggregation Rule"
            ]].dropna(how="all").to_dict(orient="records")

            source_dict = source_metadata.to_dict(orient="records")
            target_dict = target_metadata.to_dict(orient="records")

            prompt = f"""
You are a senior data engineer. Use the following metadata to generate PySpark ETL code:

- Source Metadata:
{json.dumps(source_dict, indent=2)}

- Target Metadata:
{json.dumps(target_dict, indent=2)}

- Mapping Instructions:
{json.dumps(mapping_dict, indent=2)}

Generate a Python function `transform_data(customer_df, order_df)` that applies the transformations, joins, filters, lookup logic, and aggregations.
Ensure code readability with comments explaining each block.
Return a dictionary of output DataFrames by logical table name.
"""
            response = llm.invoke([{"role": "user", "content": prompt}])
            transformation_logic = textwrap.indent(response.content.strip(), '    ')
        except Exception as e:
            print(f"Error: Failed to generate transformation logic using LLM: {e}")

    data_quality_checks = ""
    if dq_metadata is not None:
        try:
            dq_dict = dq_metadata[[
                "Validations",
                "Reject Handling",
                "Record Count Expected"
            ]].dropna(how="all").to_dict(orient="records")

            dq_prompt = f"""
You are an expert data quality analyst. Generate PySpark code to perform the following data quality checks:
{json.dumps(dq_dict, indent=2)}

Return a Python function `data_quality_checks(spark, df, table_name)` that:
- Applies validation rules
- Implements reject handling strategies
- Validates against expected record counts (thresholds)
- Returns cleaned_df, quarantine_df, and record_count_valid (boolean)
Include comments for each check and action.
"""
            response = llm.invoke([{"role": "user", "content": dq_prompt}])
            data_quality_checks = textwrap.indent(response.content.strip(), '    ')
        except Exception as e:
            print(f"Error: Failed to generate data quality checks using LLM: {e}")

    pyspark_code = f"""
# ETL Code Generated by Agentic ETL Generator

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

def transform_data(customer_df, order_df):
{transformation_logic if transformation_logic else '    # No transformation logic generated'}

def data_quality_checks(spark, df, table_name):
{data_quality_checks if data_quality_checks else '    # No data quality checks generated'}

def main():
    spark = SparkSession.builder.appName("ETL Job").getOrCreate()

    customer_df = spark.read.csv("path/to/customer.csv", header=True, inferSchema=True)
    order_df = spark.read.csv("path/to/orders.csv", header=True, inferSchema=True)

    transformed_data = transform_data(customer_df, order_df)

    for table_name, df in transformed_data.items():
        cleaned_df, quarantine_df, record_count_valid = data_quality_checks(spark, df, table_name)

        if cleaned_df is not None and record_count_valid:
            cleaned_df.write.mode("overwrite").saveAsTable(f"target_{{table_name}}")
        if quarantine_df is not None:
            quarantine_df.write.mode("append").saveAsTable("quarantine_records")

    spark.stop()

if __name__ == "__main__":
    main()
"""

    try:
        optimization_prompt = f"""
You are an expert PySpark engineer. Optimize the following PySpark code for running on {target_platform}:

{pyspark_code}

Make any platform-specific changes, especially around session creation, I/O, and performance best practices.
Return only the optimized Python code.
"""
        response = llm.invoke([{"role": "user", "content": optimization_prompt}])
        pyspark_code = response.content.strip()
    except Exception as e:
        print(f"Warning: Could not optimize for {target_platform}. Returning base code. Error: {e}")

    return pyspark_code
